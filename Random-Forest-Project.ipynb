{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  \\\n",
       "PassengerId                     \n",
       "2                   1       1   \n",
       "4                   1       1   \n",
       "7                   0       1   \n",
       "11                  1       3   \n",
       "12                  1       1   \n",
       "\n",
       "                                                          Name     Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "7                                      McCarthy, Mr. Timothy J    male  54.0   \n",
       "11                             Sandstrom, Miss. Marguerite Rut  female   4.0   \n",
       "12                                    Bonnell, Miss. Elizabeth  female  58.0   \n",
       "\n",
       "             SibSp  Parch    Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                  \n",
       "2                1      0  PC 17599  71.2833   C85        C  \n",
       "4                1      0    113803  53.1000  C123        S  \n",
       "7                0      0     17463  51.8625   E46        S  \n",
       "11               1      1   PP 9549  16.7000    G6        S  \n",
       "12               0      0    113783  26.5500  C103        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', index_col=0)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "df.drop('cabin',axis =1,inplace=True)\n",
    "df.drop('ticket',axis =1,inplace=True)\n",
    "df['age'].fillna(df['age'].mean(), inplace = True)\n",
    "embark = pd.get_dummies(df['embarked'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect the df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "2      1\n",
       "4      1\n",
       "7      0\n",
       "11     1\n",
       "12     1\n",
       "      ..\n",
       "872    1\n",
       "873    0\n",
       "880    1\n",
       "888    1\n",
       "890    1\n",
       "Name: survived, Length: 183, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Selecting the target \n",
    "\n",
    "y = df['survived']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelling the sex column and turning them into numeric values\n",
    "\n",
    "df['sex']=df['sex'].replace(['male'],0)\n",
    "df['sex']=df['sex'].replace(['female'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  pclass  sex     fare\n",
       "PassengerId                            \n",
       "2            38.0       1    1  71.2833\n",
       "4            35.0       1    1  53.1000\n",
       "7            54.0       1    0  51.8625\n",
       "11            4.0       3    1  16.7000\n",
       "12           58.0       1    1  26.5500\n",
       "...           ...     ...  ...      ...\n",
       "872          47.0       1    1  52.5542\n",
       "873          33.0       1    0   5.0000\n",
       "880          56.0       1    1  83.1583\n",
       "888          19.0       1    1  30.0000\n",
       "890          26.0       1    0  30.0000\n",
       "\n",
       "[183 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Selecting input features\n",
    "\n",
    "X = df[['age', 'pclass', 'sex', 'fare']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((137, 4), (46, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Train-test-split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6715328467153284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "model = DummyClassifier(strategy='most_frequent') #initialize the model\n",
    "model.fit(X_train, y_train)   # trains the model\n",
    "model.score(X_train, y_train) # calculates accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6739130434782609"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model on the test data\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict\n",
    "\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Training the baseline model\n",
    "#Train a Decision Tree with maximum depth 3 & fit\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "m = DecisionTreeClassifier(max_depth= 3)\n",
    "m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc 0.847\n",
      "test acc  0.739\n"
     ]
    }
   ],
   "source": [
    "#calculate training and test accuracy\n",
    "print('train acc', round(m.score(X_train, y_train), 3))\n",
    "print('test acc ', round(m.score(X_test, y_test), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Building a Random Forest from Scratch\n",
    "\n",
    "df.sample(3)\n",
    "df_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tree score 0.7\n",
      "1 tree score 0.72\n",
      "2 tree score 0.74\n",
      "3 tree score 0.67\n",
      "4 tree score 0.61\n",
      "5 tree score 0.72\n",
      "6 tree score 0.74\n",
      "7 tree score 0.67\n",
      "8 tree score 0.7\n",
      "9 tree score 0.74\n",
      "10 tree score 0.74\n",
      "11 tree score 0.7\n",
      "12 tree score 0.72\n",
      "13 tree score 0.72\n",
      "14 tree score 0.74\n",
      "15 tree score 0.74\n",
      "16 tree score 0.7\n",
      "17 tree score 0.7\n",
      "18 tree score 0.7\n",
      "19 tree score 0.7\n",
      "20 tree score 0.76\n",
      "21 tree score 0.76\n",
      "22 tree score 0.76\n",
      "23 tree score 0.74\n",
      "24 tree score 0.76\n",
      "25 tree score 0.67\n",
      "26 tree score 0.7\n",
      "27 tree score 0.59\n",
      "28 tree score 0.65\n",
      "29 tree score 0.74\n",
      "30 tree score 0.76\n",
      "31 tree score 0.74\n",
      "32 tree score 0.74\n",
      "33 tree score 0.76\n",
      "34 tree score 0.76\n",
      "35 tree score 0.67\n",
      "36 tree score 0.65\n",
      "37 tree score 0.7\n",
      "38 tree score 0.74\n",
      "39 tree score 0.74\n",
      "40 tree score 0.76\n",
      "41 tree score 0.63\n",
      "42 tree score 0.83\n",
      "43 tree score 0.63\n",
      "44 tree score 0.61\n",
      "45 tree score 0.72\n",
      "46 tree score 0.63\n",
      "47 tree score 0.72\n",
      "48 tree score 0.72\n",
      "49 tree score 0.76\n",
      "50 tree score 0.7\n",
      "51 tree score 0.76\n",
      "52 tree score 0.7\n",
      "53 tree score 0.76\n",
      "54 tree score 0.76\n",
      "55 tree score 0.76\n",
      "56 tree score 0.7\n",
      "57 tree score 0.72\n",
      "58 tree score 0.74\n",
      "59 tree score 0.7\n",
      "60 tree score 0.7\n",
      "61 tree score 0.7\n",
      "62 tree score 0.7\n",
      "63 tree score 0.74\n",
      "64 tree score 0.72\n",
      "65 tree score 0.61\n",
      "66 tree score 0.76\n",
      "67 tree score 0.76\n",
      "68 tree score 0.78\n",
      "69 tree score 0.72\n",
      "70 tree score 0.76\n",
      "71 tree score 0.65\n",
      "72 tree score 0.74\n",
      "73 tree score 0.7\n",
      "74 tree score 0.72\n",
      "75 tree score 0.72\n",
      "76 tree score 0.65\n",
      "77 tree score 0.76\n",
      "78 tree score 0.7\n",
      "79 tree score 0.72\n",
      "80 tree score 0.63\n",
      "81 tree score 0.76\n",
      "82 tree score 0.74\n",
      "83 tree score 0.7\n",
      "84 tree score 0.72\n",
      "85 tree score 0.67\n",
      "86 tree score 0.76\n",
      "87 tree score 0.7\n",
      "88 tree score 0.74\n",
      "89 tree score 0.74\n",
      "90 tree score 0.72\n",
      "91 tree score 0.76\n",
      "92 tree score 0.59\n",
      "93 tree score 0.67\n",
      "94 tree score 0.67\n",
      "95 tree score 0.74\n",
      "96 tree score 0.7\n",
      "97 tree score 0.7\n",
      "98 tree score 0.67\n",
      "99 tree score 0.7\n"
     ]
    }
   ],
   "source": [
    "forest = []\n",
    "\n",
    "for i in range(100):\n",
    "    sample = df_train.sample(50)\n",
    "    X_test_sample = sample[['age', 'pclass', 'sex', 'fare']]\n",
    "    y_test_sample = sample['survived']\n",
    "    tree = DecisionTreeClassifier(max_depth = 3)\n",
    "    tree.fit(X_test_sample, y_test_sample)\n",
    "    forest.append(tree)\n",
    "    print(i, 'tree score', round(tree.score(X_test, y_test), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7956204379562044,\n",
       " 0.7737226277372263,\n",
       " 0.8248175182481752,\n",
       " 0.781021897810219,\n",
       " 0.6934306569343066,\n",
       " 0.8102189781021898,\n",
       " 0.8248175182481752,\n",
       " 0.7591240875912408,\n",
       " 0.7883211678832117,\n",
       " 0.7956204379562044,\n",
       " 0.7956204379562044,\n",
       " 0.7956204379562044,\n",
       " 0.781021897810219,\n",
       " 0.781021897810219,\n",
       " 0.781021897810219,\n",
       " 0.7883211678832117,\n",
       " 0.7518248175182481,\n",
       " 0.7883211678832117,\n",
       " 0.781021897810219,\n",
       " 0.8102189781021898,\n",
       " 0.8175182481751825,\n",
       " 0.8029197080291971,\n",
       " 0.8321167883211679,\n",
       " 0.8175182481751825,\n",
       " 0.7883211678832117,\n",
       " 0.7518248175182481,\n",
       " 0.7372262773722628,\n",
       " 0.7372262773722628,\n",
       " 0.7153284671532847,\n",
       " 0.8029197080291971,\n",
       " 0.7883211678832117,\n",
       " 0.8248175182481752,\n",
       " 0.8102189781021898,\n",
       " 0.7883211678832117,\n",
       " 0.8321167883211679,\n",
       " 0.7664233576642335,\n",
       " 0.7299270072992701,\n",
       " 0.7664233576642335,\n",
       " 0.7883211678832117,\n",
       " 0.7883211678832117,\n",
       " 0.8029197080291971,\n",
       " 0.7591240875912408,\n",
       " 0.7153284671532847,\n",
       " 0.7007299270072993,\n",
       " 0.7737226277372263,\n",
       " 0.8029197080291971,\n",
       " 0.7372262773722628,\n",
       " 0.8029197080291971,\n",
       " 0.7956204379562044,\n",
       " 0.7883211678832117,\n",
       " 0.7883211678832117,\n",
       " 0.7883211678832117,\n",
       " 0.7518248175182481,\n",
       " 0.8029197080291971,\n",
       " 0.8102189781021898,\n",
       " 0.7883211678832117,\n",
       " 0.781021897810219,\n",
       " 0.7737226277372263,\n",
       " 0.8029197080291971,\n",
       " 0.8029197080291971,\n",
       " 0.7372262773722628,\n",
       " 0.7883211678832117,\n",
       " 0.8102189781021898,\n",
       " 0.7956204379562044,\n",
       " 0.8029197080291971,\n",
       " 0.7956204379562044,\n",
       " 0.8321167883211679,\n",
       " 0.7883211678832117,\n",
       " 0.781021897810219,\n",
       " 0.7956204379562044,\n",
       " 0.8029197080291971,\n",
       " 0.7518248175182481,\n",
       " 0.8029197080291971,\n",
       " 0.7883211678832117,\n",
       " 0.7883211678832117,\n",
       " 0.781021897810219,\n",
       " 0.7591240875912408,\n",
       " 0.8248175182481752,\n",
       " 0.781021897810219,\n",
       " 0.7737226277372263,\n",
       " 0.7153284671532847,\n",
       " 0.7883211678832117,\n",
       " 0.7591240875912408,\n",
       " 0.7299270072992701,\n",
       " 0.8248175182481752,\n",
       " 0.7591240875912408,\n",
       " 0.7956204379562044,\n",
       " 0.8029197080291971,\n",
       " 0.7956204379562044,\n",
       " 0.8175182481751825,\n",
       " 0.7737226277372263,\n",
       " 0.7956204379562044,\n",
       " 0.7883211678832117,\n",
       " 0.7372262773722628,\n",
       " 0.7445255474452555,\n",
       " 0.708029197080292,\n",
       " 0.7956204379562044,\n",
       " 0.7664233576642335,\n",
       " 0.7372262773722628,\n",
       " 0.7664233576642335]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Calculate a list of training scores for all trees on the full training set\n",
    "trains = [tree.score(X_train, y_train) for tree in forest]\n",
    "trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train acc  0.781\n"
     ]
    }
   ],
   "source": [
    "##Calculate the mean training score\n",
    "import numpy as np\n",
    "print('mean train acc ', round(np.mean(trains), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test acc  0.712\n"
     ]
    }
   ],
   "source": [
    "##Calculate the mean test score in the same way\n",
    "tests = [tree.score(X_test, y_test) for tree in forest]\n",
    "print('mean test acc ', round(np.mean(tests), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9   ...  36  37  38  39  40  41  42  \\\n",
       "0    1   1   1   0   1   1   1   1   1   1  ...   1   0   1   0   1   1   1   \n",
       "1    1   1   1   0   1   1   1   1   1   1  ...   1   1   1   0   1   1   1   \n",
       "2    1   1   0   0   1   1   1   1   0   1  ...   1   0   1   0   0   1   1   \n",
       "3    1   1   0   0   1   0   1   0   0   1  ...   1   0   1   0   1   0   0   \n",
       "4    1   1   0   0   0   0   1   0   1   1  ...   1   0   1   0   0   1   1   \n",
       "..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "95   1   1   1   1   0   1   1   1   0   1  ...   1   1   1   1   1   0   1   \n",
       "96   1   1   0   0   1   0   1   0   1   1  ...   1   0   1   0   0   0   0   \n",
       "97   1   1   0   0   1   0   1   1   0   1  ...   1   0   1   0   0   0   0   \n",
       "98   1   1   0   0   0   0   1   1   0   1  ...   1   0   1   0   0   0   0   \n",
       "99   1   1   0   0   1   1   1   0   0   1  ...   1   0   1   0   0   0   1   \n",
       "\n",
       "    43  44  45  \n",
       "0    1   1   0  \n",
       "1    1   1   0  \n",
       "2    1   1   0  \n",
       "3    1   0   0  \n",
       "4    1   0   0  \n",
       "..  ..  ..  ..  \n",
       "95   1   0   0  \n",
       "96   1   0   0  \n",
       "97   1   0   0  \n",
       "98   1   0   0  \n",
       "99   1   1   0  \n",
       "\n",
       "[100 rows x 46 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Majority Vote\n",
    "# Create a list of predictions for every tree\n",
    "\n",
    "pred = [tree.predict(X_test) for tree in forest]\n",
    "all_pred = pd.DataFrame(pred)\n",
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority vote test score: 0.739\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy from most frequent prediction on each data point\n",
    "y_pred = all_pred.mode().T\n",
    "overall_accuracy = accuracy_score(y_pred, y_test)\n",
    "print('majority vote test score:', round(overall_accuracy,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority vote test score: 0.739\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = accuracy_score(y_pred, y_test)\n",
    "print('majority vote test score:', round(overall_accuracy,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####RandomForest with Scikit#######\n",
    "rand = RandomForestClassifier(n_estimators=100, max_depth=3)   \n",
    "rand.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.839"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rand.score(X_train, y_train),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rand.score(X_test, y_test),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rand.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.53      0.59        15\n",
      "           1       0.79      0.87      0.83        31\n",
      "\n",
      "    accuracy                           0.76        46\n",
      "   macro avg       0.73      0.70      0.71        46\n",
      "weighted avg       0.75      0.76      0.75        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rand.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 age, Score: 0.25749\n",
      "              pclass, Score: 0.02875\n",
      "                 sex, Score: 0.48737\n",
      "                fare, Score: 0.22638\n"
     ]
    }
   ],
   "source": [
    "names= ('age', 'pclass', 'sex', 'fare')\n",
    "# summarize feature importance\n",
    "for i,v in zip(names, importance):\n",
    "        print('%20s, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.bar([x for x in range(len(importance))], importance, alpha=0.5 )\n",
    "plt.title('Random Forest Feature Importance')\n",
    "\n",
    "\n",
    "plt.xticks(range(4), ('age', 'pclass', 'sex', 'fare'))               \n",
    "                                                                  \n",
    "plt.ylabel('Survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  7]\n",
      " [ 4 27]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix( y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1f449c64ca0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(rand, X_test, y_test, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\49178\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from mlxtend) (1.19.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from mlxtend) (0.23.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\49178\\anaconda3\\lib\\site-packages (from mlxtend) (50.3.1.post20201107)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from mlxtend) (0.17.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from mlxtend) (3.3.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\49178\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\49178\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa=X.to_numpy()\n",
    "Xa\n",
    "ya=y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\mlxtend\\plotting\\decision_regions.py:244: UserWarning: No contour levels were found within the data range.\n",
      "  ax.contour(xx, yy, Z, cset.levels,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "feat_labels = ['age', 'pclass', 'sex', 'fare']\n",
    "for clf in zip([rand]):                \n",
    "    rand.fit(Xa, ya)\n",
    "    fig = plot_decision_regions(Xa, ya, rand, filler_feature_values={0: 4000, 1: 4000, 2: 4000, 3: 4000},\n",
    "                      filler_feature_ranges={0: 8000, 1: 8000, 2: 8000, 3: 8000}, \n",
    "                                legend=4) \n",
    "cf = plt.contourf(Xa,ya, levels=[10, 50, 80], cmap=cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.title('Decision Boundary')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\49178\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\49178\\anaconda3\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\49178\\anaconda3\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=xgboost.XGBClassifier(tree_method='hist')   #call a classifier\n",
    "# tree method is a tree construction algorithm used in XGBoost\n",
    "#XGBoost supports approx, hist and gpu_hist for distributing trainig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV] min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5, score=0.875, total=   0.1s\n",
      "[CV] min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5, score=0.890, total=   0.1s\n",
      "[CV] min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5, score=0.772, total=   0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5, score=0.696, total=   0.0s\n",
      "[CV] min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=10, learning_rate=0.2, gamma=0.4, colsample_bytree=0.5, score=0.880, total=   0.0s\n",
      "[CV] min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6, score=0.858, total=   0.1s\n",
      "[CV] min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6, score=0.882, total=   0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6, score=0.768, total=   0.1s\n",
      "[CV] min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6, score=0.655, total=   0.0s\n",
      "[CV] min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=8, max_depth=12, learning_rate=0.25, gamma=0.5, colsample_bytree=0.6, score=0.887, total=   0.1s\n",
      "[CV] min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4, score=0.853, total=   0.1s\n",
      "[CV] min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4 \n",
      "[15:04:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4, score=0.820, total=   0.1s\n",
      "[CV] min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4, score=0.762, total=   0.1s\n",
      "[CV] min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4, score=0.727, total=   0.0s\n",
      "[CV] min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  min_child_weight=6, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.4, score=0.866, total=   0.0s\n",
      "[CV] min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.840, total=   0.1s\n",
      "[CV] min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.780, total=   0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.792, total=   0.1s\n",
      "[CV] min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.740, total=   0.0s\n",
      "[CV] min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=8, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.870, total=   0.0s\n",
      "[CV] min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5, score=0.880, total=   0.1s\n",
      "[CV] min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5, score=0.820, total=   0.1s\n",
      "[CV] min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5, score=0.765, total=   0.1s\n",
      "[CV] min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5, score=0.665, total=   0.1s\n",
      "[CV] min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5 \n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  min_child_weight=4, max_depth=12, learning_rate=0.1, gamma=0.5, colsample_bytree=0.5, score=0.861, total=   0.1s\n",
      "[15:04:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    1.4s finished\n",
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                                           random_state=None, reg_alpha=None,\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method='hist',\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.6],\n",
       "                                        'gamma': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
       "                                        'learning_rate': [0.1, 0.15, 0.2, 0.25,\n",
       "                                                          0.3],\n",
       "                                        'max_depth': [6, 8, 10, 12, 15],\n",
       "                                        'min_child_weight': [2, 4, 6, 8]},\n",
       "                   scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set all possible parameters to find a best combination\n",
    "\n",
    "params={\"learning_rate\":[0.10,0.15,0.20,0.25,0.30],   #how fast we want our model to move \n",
    "        \"max_depth\":[6,8,10,12,15],               #the maximum tree depth\n",
    "        \"min_child_weight\":[2,4,6,8],                         #min sum of distance\n",
    "        \"gamma\":[0.1,0.2,0.3,0.4,0.5],      #min loss reduction required to make a further partition\n",
    "                                             #on a leaf node of the tree\n",
    "        \"colsample_bytree\":[0.3,0.4,0.5,0.6]       #is the sub sample ratio of \n",
    "                                               #columns while constructing each tree\n",
    "       }\n",
    "\n",
    "clf=RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, scoring='roc_auc',cv=5, verbose=3)\n",
    "#randomized search on hyper parameters, that gives us the best possible combination, \n",
    "#the cross validated search over parameters\n",
    "#scoring= so it's the ideal point between the true positives and the false positives\n",
    "#cv=Determines the cross-validation splitting strategy. \n",
    "#Controls the verbosity: the higher, the more messages.\n",
    "\n",
    "#fitting the model\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 8,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.2,\n",
       " 'gamma': 0.4,\n",
       " 'colsample_bytree': 0.5}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_   #find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.4, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.2, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=8, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run another xgboost classifier and then feed these parameters down here  \n",
    "# this function over here makes it easy to test it again . \n",
    "\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49178\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1f44a656430>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fitting the model with best parameter\n",
    "#copy-paste of previous best model       \n",
    "final_model= xgboost.XGBClassifier(base_score=None, booster='gbtree', colsample_bylevel=None,\n",
    "              colsample_bynode=None, colsample_bytree=0.5, gamma=0.5,\n",
    "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
    "              learning_rate=0.1, max_delta_step=None, max_depth=15,\n",
    "              min_child_weight=8, monotone_constraints=None,\n",
    "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
    "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
    "              scale_pos_weight=None, subsample=None, tree_method='hist',\n",
    "              validate_parameters=None, verbosity=None)\n",
    "\n",
    "#fitting\n",
    "cm=final_model.fit(X, y )\n",
    "\n",
    "#make a prediction based on the final model \n",
    "y_pred1 = cm.predict(X)\n",
    "pred_xgboost=final_model.predict(X)\n",
    "\n",
    "#plot with better/improved results\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(final_model, X, y, cmap=plt.cm.Blues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
